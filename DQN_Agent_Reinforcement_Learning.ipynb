{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy tensorflow faiss-cpu matplotlib"
      ],
      "metadata": {
        "id": "2HVKDxhw7zzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import faiss\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "i3NkACHzTIR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Maze Environment\n",
        "class MazeEnvironment:\n",
        "    def __init__(self, size=10):\n",
        "        self.size = size\n",
        "        self.maze = np.zeros((size, size), dtype=int)  # 0: free, 1: wall\n",
        "        self.start = (1, 1)\n",
        "        self.goal = (size-2, size-2)\n",
        "        self.agent_pos = self.start\n",
        "        self.generate_maze()\n",
        "\n",
        "    def generate_maze(self):\n",
        "        num_walls = int(0.25 * self.size * self.size)\n",
        "        for _ in range(num_walls):\n",
        "            x, y = np.random.randint(0, self.size, 2)\n",
        "            if (x, y) != self.start and (x, y) != self.goal:\n",
        "                self.maze[x, y] = 1\n",
        "        self.maze[self.start] = 0\n",
        "        self.maze[self.goal] = 0\n",
        "\n",
        "    def get_state(self):\n",
        "        x, y = self.agent_pos\n",
        "        state = np.zeros((3, 3))\n",
        "        for i in range(-1, 2):\n",
        "            for j in range(-1, 2):\n",
        "                xi, yj = x + i, y + j\n",
        "                if 0 <= xi < self.size and 0 <= yj < self.size:\n",
        "                    state[i+1, j+1] = self.maze[xi, yj]\n",
        "                else:\n",
        "                    state[i+1, j+1] = 1\n",
        "        return np.array([x, y] + state.flatten().tolist(), dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
        "        dx, dy = moves[action]\n",
        "        new_pos = (self.agent_pos[0] + dx, self.agent_pos[1] + dy)\n",
        "\n",
        "        reward = -0.1\n",
        "        done = False\n",
        "\n",
        "        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and\n",
        "                self.maze[new_pos] != 1):\n",
        "            self.agent_pos = new_pos\n",
        "\n",
        "        if self.agent_pos == self.goal:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = self.start\n",
        "        return self.get_state()\n",
        "\n",
        "# DQN Model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = DQN(state_size, action_size).to(self.device)\n",
        "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0005)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.update_target_model()\n",
        "        self.faiss_index = faiss.IndexFlatL2(state_size)\n",
        "        self.faiss_memory = deque(maxlen=10000)\n",
        "        self.faiss_id = 0\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        state_vec = state.astype(np.float32)\n",
        "        if len(self.faiss_memory) >= 10000:\n",
        "            self.faiss_index.reset()\n",
        "            vectors = np.array([m[1].astype(np.float32) for m in self.faiss_memory])\n",
        "            self.faiss_index.add(vectors)\n",
        "        self.faiss_index.add(np.array([state_vec]))\n",
        "        self.faiss_memory.append((self.faiss_id, state, action, reward, next_state, done))\n",
        "        self.faiss_id += 1\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        return q_values.argmax().item()\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = torch.FloatTensor(np.array([t[0] for t in minibatch])).to(self.device)\n",
        "        actions = torch.LongTensor([t[1] for t in minibatch]).to(self.device)\n",
        "        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array([t[3] for t in minibatch])).to(self.device)\n",
        "        dones = torch.FloatTensor([t[4] for t in minibatch]).to(self.device)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        next_q_values = self.target_model(next_states)\n",
        "        targets = q_values.clone()\n",
        "        for i in range(batch_size):\n",
        "            targets[i, actions[i]] = rewards[i] + self.gamma * next_q_values[i].max() * (1 - dones[i])\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_fn(q_values, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # FAISS-based replay\n",
        "        if self.faiss_index.ntotal > 0:\n",
        "            state_sample = states[np.random.randint(0, batch_size)].cpu().numpy()\n",
        "            D, I = self.faiss_index.search(np.array([state_sample.astype(np.float32)]), k=1)\n",
        "            if I[0][0] >= 0:\n",
        "                idx = I[0][0]\n",
        "                _, s, a, r, ns, d = self.faiss_memory[idx]\n",
        "                s = torch.FloatTensor(s).unsqueeze(0).to(self.device)\n",
        "                ns = torch.FloatTensor(ns).unsqueeze(0).to(self.device)\n",
        "                with torch.no_grad():\n",
        "                    next_q = self.target_model(ns).max().item()\n",
        "                target = r if d else r + self.gamma * next_q\n",
        "                q_val = self.model(s)\n",
        "                target_q = q_val.clone()\n",
        "                target_q[0, a] = target\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.loss_fn(q_val, target_q)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Training\n",
        "def train_dqn(episodes=500):\n",
        "    env = MazeEnvironment(size=10)\n",
        "    state_size = 2 + 3*3\n",
        "    action_size = 4\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    batch_size = 64\n",
        "    success_count = 0\n",
        "    last_rewards = deque(maxlen=20)\n",
        "    start_time = time.time()\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        for step in range(200):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                success_count += 1\n",
        "                break\n",
        "\n",
        "        agent.replay(batch_size)\n",
        "        if (e + 1) % 10 == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        last_rewards.append(total_reward)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Episode: {e+1}/{episodes}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}, \"\n",
        "              f\"Successes: {success_count}, Time: {elapsed_time:.0f}s\")\n",
        "\n",
        "        if len(last_rewards) == 20 and sum(1 for r in last_rewards if r > 5) >= 18:\n",
        "            print(\"Early stopping: Agent consistently reaches goal.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training completed in {time.time() - start_time:.0f} seconds.\")\n",
        "    return env, agent\n",
        "\n",
        "# Visualization\n",
        "def visualize_path(env, agent):\n",
        "    path = []\n",
        "    state = env.reset()\n",
        "    env.maze[env.goal] = 2\n",
        "    done = False\n",
        "    steps = 0\n",
        "    max_steps = 200\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        path.append(env.agent_pos)\n",
        "        action = agent.act(state)\n",
        "        state, _, done = env.step(action)\n",
        "        steps += 1\n",
        "\n",
        "    maze_display = env.maze.copy()\n",
        "    for pos in path:\n",
        "        if maze_display[pos] not in [2]:\n",
        "            maze_display[pos] = 3\n",
        "    maze_display[env.start] = 4\n",
        "\n",
        "    cmap = ListedColormap(['white', 'black', 'red', 'blue', 'green'])\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(maze_display, cmap=cmap, interpolation='none')\n",
        "    start_row, start_col = env.start\n",
        "    goal_row, goal_col = env.goal\n",
        "    plt.text(start_col, start_row, 'S', ha='center', va='center', color='black', fontsize=12, fontweight='bold')\n",
        "    plt.text(goal_col, goal_row, 'G', ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='white', label='Free'),\n",
        "        Patch(facecolor='black', label='Wall'),\n",
        "        Patch(facecolor='red', label='Goal'),\n",
        "        Patch(facecolor='blue', label='Path'),\n",
        "        Patch(facecolor='green', label='Start')\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "    plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
        "    plt.title(\"10x10 Maze with Agent's Path\")\n",
        "    plt.show()\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    env, agent = train_dqn(episodes=1000)\n",
        "    visualize_path(env, agent)"
      ],
      "metadata": {
        "id": "00DfYnBC8XJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute grid cell size in pixels\n",
        "def get_grid_cell_size(fig_width_inch=8, grid_size=10, dpi=None):\n",
        "    # Get DPI from matplotlib if not provided\n",
        "    if dpi is None:\n",
        "        fig = plt.figure(figsize=(fig_width_inch, fig_width_inch))\n",
        "        dpi = fig.dpi\n",
        "        plt.close(fig)\n",
        "    # Calculate pixel size of figure\n",
        "    fig_width_px = fig_width_inch * dpi\n",
        "    # Calculate pixel size of one grid cell\n",
        "    cell_size_px = fig_width_px / grid_size\n",
        "    return int(cell_size_px)\n",
        "\n",
        "# Function to resize PNG image to grid cell size\n",
        "def resize_image(image_path, cell_size_px):\n",
        "    img = Image.open(image_path)\n",
        "    # Resize to cell_size_px x cell_size_px, maintaining aspect ratio with padding if needed\n",
        "    img = img.resize((cell_size_px, cell_size_px), Image.LANCZOS)\n",
        "    # Convert to RGBA if not already\n",
        "    if img.mode != 'RGBA':\n",
        "        img = img.convert('RGBA')\n",
        "    return img\n",
        "\n",
        "def save_frames_as_images(env, agent, output_dir=\"frames\", start_img_path='start.png',\n",
        "                          goal_img_path='goal.png', agent_img_path='agent.png'):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    state = env.reset()\n",
        "    maze_display = env.maze.copy()  # Base maze with 0 (free) and 1 (walls)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    max_steps = 200\n",
        "\n",
        "    # Compute grid cell size\n",
        "    cell_size_px = get_grid_cell_size(fig_width_inch=8, grid_size=10)\n",
        "\n",
        "    # Load and resize images\n",
        "    start_img = resize_image(start_img_path, cell_size_px)\n",
        "    goal_img = resize_image(goal_img_path, cell_size_px)\n",
        "    agent_img = resize_image(agent_img_path, cell_size_px)\n",
        "\n",
        "    # Set up plot\n",
        "    cmap = ListedColormap(['white', 'black'])  # Only for maze (free/walls)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "    while not done and steps < max_steps:\n",
        "        # Plot maze\n",
        "        ax.clear()\n",
        "        ax.imshow(maze_display, cmap=cmap, interpolation='none')\n",
        "\n",
        "        # Place start image\n",
        "        start_row, start_col = env.start\n",
        "        ax.imshow(start_img, extent=(start_col - 0.5, start_col + 0.5, start_row + 0.5, start_row - 0.5), zorder=2)\n",
        "\n",
        "        # Place goal image\n",
        "        goal_row, goal_col = env.goal\n",
        "        ax.imshow(goal_img, extent=(goal_col - 0.5, goal_col + 0.5, goal_row + 0.5, goal_row - 0.5), zorder=2)\n",
        "\n",
        "        # Place agent image at current position\n",
        "        agent_row, agent_col = env.agent_pos\n",
        "        ax.imshow(agent_img, extent=(agent_col - 0.5, agent_col + 0.5, agent_row + 0.5, agent_row - 0.5), zorder=1)\n",
        "\n",
        "        # Add grid and labels\n",
        "        ax.grid(color='gray', linestyle='--', linewidth=0.5)\n",
        "        ax.set_xticks(np.arange(0, 10, 1))\n",
        "        ax.set_yticks(np.arange(0, 10, 1))\n",
        "        ax.set_title(f\"Agent Navigation in 10x10 Maze - Step {steps}\")\n",
        "\n",
        "        # Save the frame as an image\n",
        "        frame_path = os.path.join(output_dir, f\"frame_{steps:03d}.png\")\n",
        "        fig.canvas.draw()\n",
        "        plt.savefig(frame_path, bbox_inches='tight', dpi=100)\n",
        "        print(f\"Saved frame: {frame_path}\")\n",
        "\n",
        "        # Take action\n",
        "        action = agent.act(state)\n",
        "        state, _, done = env.step(action)\n",
        "        steps += 1\n",
        "\n",
        "    # Close plot\n",
        "    plt.close(fig)\n",
        "    print(f\"All frames saved in {output_dir}\")\n"
      ],
      "metadata": {
        "id": "EGBE_fBKOw8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #save each step as an image\n",
        "save_frames_as_images(\n",
        "    env,\n",
        "    agent,\n",
        "    output_dir=\"/content/drive/MyDrive/maze_frames\",\n",
        "    start_img_path='/content/drive/MyDrive/merry_go-removebg-preview.png',\n",
        "    goal_img_path='/content/drive/MyDrive/gomu_2-removebg-preview.png',\n",
        "    agent_img_path='/content/drive/MyDrive/luffy_agent.png'\n",
        ")"
      ],
      "metadata": {
        "id": "TT1VIlKn9QBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the saved images to gif\n",
        "output_dir = \"/content/drive/MyDrive/maze_frames\"\n",
        "gif_path = \"/content/drive/MyDrive/maze_animation_new.gif\"\n",
        "images = []\n",
        "for file_path in sorted(glob.glob(os.path.join(output_dir, \"frame_*.png\"))):\n",
        "    images.append(imageio.imread(file_path))\n",
        "imageio.mimsave(gif_path, images, fps=2)\n",
        "print(f\"GIF saved as {gif_path}\")"
      ],
      "metadata": {
        "id": "kOjBi5QE9gLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}